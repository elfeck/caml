\input{paper-layout}

\pagestyle{empty}
\hyphenation{}

\begin{document}

\title{Data Mining with Sparse Grids}

\author{
\authorblockN{Sebastian Kreisel}
\authorblockA{Department for Informatics\\Technische Universität München\\
Email: sebastian.kreisel@tum.de}
%\and
%\authorblockN{}
%\authorblockA{}
}

\specialpapernotice{Seminar Computational Aspects of Machine Learning}

\maketitle
\setlength{\parskip}{3px}

%\thispagestyle{plain}

\begin{abstract}
  TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract TODO: Abstract 
\end{abstract}

\begin{keywords}
Sparse grids; Data mining; Hierarchical discretization; Curse of
dimensionality
\end{keywords}

\input{part_introduction}

%% Page 1, 2, 3
% Grid vs. pointbased approach of discretization
\section{Grid discretization}\label{grid}

In machine learning, algorithms usually focus on a given trainings--data set
$X$, for instance
$$ X = \{x^{(i)} | \ x^{(i)} \in [0, 1]^d\}_{i = 1}^M \ , \ \ \
Y = \{y^{(i)} | \ y^{(i)} \in \mathbb{R}\}_{i=1}^M$$
with an associated solution--set $Y$ in case of supervised learning.

\par

Grid--based approaches introduce an additional set $G$ of $N$ 
\emph{gridpoints} with
$$ G = \{1,2,\dots,N\} \ .$$
For each dimension of the feature space a separate $G$ (with possibly
different $N$) is constructed forming a grid over the feature space.
This, by the grid discretized space,
will then be used instead of working with the datapoints in the original
feature space directly.
\par
To examine sparse grids the following section will first introduce full
grid discretization.


% Gridpoints, Basis function, surplus, sum
% d > 1, tensor product
\subsection{Full grid}
To construct a \emph{full} grid we chose the gridpoints $G$ equidistant,
without gridpoints lying on the borders. Further, the space to be
discretized is assumed to be a unit--hypercube.

\par
First, let $d = 1$. Around each gridpoint $i$ we center a one--dimensional
\emph{basis function}
$$ \phi_i(x) = \max\{0, 1 - |(N + 1)x - i|\} \ .$$
$\phi_i(x)$ is the standard hat--function, centered around $i$ and dilated
to have local support between the gridpoints $i - 1$ and $i + 1$. \ref{..}
shows $G = \{1,2,\dots,7\}$ and the related basis--functions.

\par

To discretize a function $f(x)$ we introduce a coefficient (surplus)
$\alpha_i$ for each gridpoint $i$. This coefficient is defined to be
$f$ evaluated at the gridpoint $i$
$$\alpha_i = f(\frac{i}{N+1}) \ .$$
Taking the sum
$$ f(x) \approx  \hat{f}(x) = \sum_{i \in G}{\alpha_i \phi_i(x)} $$
over all weighted basis--functions $\phi_i$ discretizes (approximates) $f$.
\ref{..} illustrates this.

\par

For $f(\vec{x})$ with $d > 1$, each dimension $j$ is treated separately 
using the above
scheme. This results in $d$ one--dimensional functions
$\hat{f}_j(x_j)$ with $x_j$ denoting the $j$--th
element in the vector $\vec{x}$. Taking the tensor product
$$f(\vec{x}) \approx \hat{f}(\vec{x}) = \prod_{j = 1}^{d}{\hat{f}_j(x_j)} $$
approximates $f(\hat{x})$.

\subsection{Hierachial basis functions}
\begin{itemize}
\item Basic notion
\item Hierachial surplus
\item Hierachial subspaces
\end{itemize}

\subsection{Sparse grid}
\begin{itemize}
\item Disregarding subspaces
\item Trade-off
\item Spartial adaptivity
\item Boundry and smoothness note
\end{itemize}

% Page 4
\section{Sparse grids in Machine Learning}
\begin{itemize}
\item Quick note on classification/regression
\item Least squares
\item Least squarse with sparse grids
\item Matrix formulation
\item Notes on matrix solving etc.
\end{itemize}

% Page 5
\section{Something something Implemenation}
\begin{itemize}
\item
\end{itemize}

% Page 5 (, 6)
\section{Conclusion}
\begin{itemize}
\item
\end{itemize}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section
% NOTE: BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/

% can use a bibliography generated by BibTeX as a .bbl file
% standard IEEE bibliography style from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/IEEEtran/bibtex
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,references}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{ref:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 	End of the document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}