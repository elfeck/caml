% Page 4
\section{Sparse Grids in Machine Learning}

As mentioned in the introduction, data mining scenarios with large datasets
and high-dimensional data are in general difficult to deal with.
Areas of application
that show these characteristics include engineering (e.g. heat-transfer
modeling) \cite{disspeh},
machine learning and data mining (e.g. estimation of cosmological redshifts
and option pricing in finance) \cite{disspfl}. In this section
we apply sparse grids to machine learning tasks
(classification and regression) relevant for data mining.
\par
\subsection{Least squares estimation}
A commonly used method in machine learning is \emph{least squares estimation}
$$\hat{c} = \underset{f}{\text{arg \hspace{-0.5mm}min}}\Bigg(\frac{1}{M}\sum\limits_{i = 1}^{M}{\Big(y^{(i)} - f(x^{(i)})\Big)^2 \ + \ \lambda R(f)} \Bigg)$$ \
using the notation for training data 
introduced in Sec.~\ref{sec:grid}. With this formula we
derive an estimator $\hat{c}$ that is the function which models the
data $X \times Y$ with the least error (minimization over $f$).
The term $R(f)$ is a regularization term to enforce smoothness. A common choice
is the $L_2$-Norm of the gradient of $f$: $R(f) = ||\nabla f||_2$.
\par
We will now use least squares estimation in a sparse grid setting. Intuitively,
$f$ and ultimately $\hat{c}$
 are functions discretized on a sparse grid and represented
accordingly:
$$\hat{c} = \underset{{\alpha}}{\text{arg \hspace{-0.5mm}min}}\Bigg(\frac{1}{M}
\sum\limits_{i = 1}^{M}{\Big(y^{(j)} - {\sum_{j=1}^N{\alpha_j}\phi_j(x^{(j)})}\Big)^2 +
  \lambda {\sum_{j=1}^N{\alpha_j^2}}}\color{black} \Bigg) \ .$$
Note, that for better readability the level-index $l$ is omitted here and
we now simply enumerate all grid points, basis functions $\phi_j$ and surpluses
$\alpha_j$
 by the index $j \in \{1..N\}$.
We now minimize with respect to the hierarchical surpluses $\alpha_j$,
which are the defining factor for $\hat{c}$.
When minimizing with respect to coefficients, a common choice for
$R(f)$ is $\sum_j^N{\alpha_j^2}$. Compared to the gradient the
expressions in the following get simplified using this regularization
\cite{disspfl}.
\subsection{Matrix formulation}
Minimizing with respect to $\alpha$ leads to a system of linear equations that
can be formulated using the matrix expression
$$\Big(\frac{1}{M} BB^T + \lambda I \Big)\vec{\alpha} = \frac{1}{M}B\vec{y} \
.$$
The matrix
$$ \mathbb{R}^{N \times M} \ni B =
\begin{bmatrix}
  \phi_1(x^{(1)}) & \dots & \phi_1(x^{(M)}) \\
  \vdots & \ddots & \vdots \\
  \phi_N(x^{(1)}) & \dots  & \phi_N(x^{(M)}) \\
\end{bmatrix}
$$
sets the grid points in relation to the data points in $X$. By solving this
positive definite system we obtain the unique surplus-vector
$\vec{\alpha}$, defining $\hat{c}$ \cite{disshei}.
\par
The
matrix product $BB^T \in R^{N \times N}$ demonstrates that the number of freedoms
is not dependent on the number of data points. Thus, the computational effort
scales only linearly with respect to the number of data points $M$ \cite{disspfl}. Because
the matrix $B$ might have a lot of entries and is not sparse, methods like
Conjugate Gradients can be used to efficiently solve this system
of linear equations \cite{disshei}.

\subsection{Testing sparse grids}
In order to get a good solution several factors have to be determined before
sparse grids can be employed. The regularization parameter $\lambda$ and the
amount of refinement steps can be determined using cross-validation.
A straightforward way to refine grid points is to choose candidates
with high surpluses $\alpha$ as discussed in Sec.~\ref{subsec:ada}. For more
details on the refining strategy refer to \cite{disspfl}.
\par
First we examine the results of the checker-board dataset, a highly non-linear
classification test set (see FIGURE).
While containing no noise, the checker-board-structured
distribution of points contains very steep areas, which are difficult to model.
Starting with a maximal level of $n=4$ (49 grid points)
the grid begins to capture the structure of the data with approximately
70\% correct classifications.
With a level of $n>7$ \ (796 grid points) more than 98\% are classified
correctly \cite{disspfl}. This demonstrates the ability of sparse grids
to model difficult non-linear datasets.
\par
The second problem we examine in the following is a real world regression
task.
A phenomenon similar to the doppler-effect called cosmological redshift
is modeled. The training data consists of photometric measurements from known
astronomical objects. By applying regression the redshift of
newly observed objects is to be estimated using sparse grids.
The dataset available contains 60,000 data points with 5 dimensions. Even though
the shape of the data is inherently difficult to model, adaptive refinement
helps to capture the core features. In contrast to the previous
example, drastically more grid points are required to archive a low mean
squared error. At approximately 20,000 grid points the error drops
considerably. For more detailed examination refer to \cite{disspfl}.
\par
Those two examples demonstrate that sparse grids are able to cope with
non-linear, large and moderately high dimensional datasets. As previously
mentioned, spatial adaptivity is crucial to capture the features of the
problem and enables sparse grids to be a competitive method for data mining
and machine learning.


%\begin{itemize}
%\item Quick note on classification/regression
%\item Least squares
%\item Least squarse with sparse grids
%\item Matrix formulation
%\item Notes on matrix solving etc.
%\end{itemize

%%% Local Variables:
%%% TeX-master: "report"
%%% End:
